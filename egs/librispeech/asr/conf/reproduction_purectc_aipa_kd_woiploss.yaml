arch: s2t_ctc
encoder-type: transformer

optimizer: adam
clip-norm: 10.0
lr-scheduler: inverse_sqrt
warmup-init-lr: 1e-7
warmup-updates: 10000
lr: 2e-3
adam_betas: (0.9,0.98)

criterion: ctc
zero_infinity: True
ctc-weight: 1.0

encoder-normalize-before: True
decoder-normalize-before: True

subsampling-type: conv1d
subsampling-layers: 2
subsampling-filter: 1024
subsampling-kernel: 5
subsampling-stride: 2
subsampling-norm: none
subsampling-activation: glu

dropout: 0.1
activation-fn: relu
encoder-embed-dim: 256
encoder-ffn-embed-dim: 2048
encoder-layers: 18
encoder-attention-heads: 4

# Append-based Interpolation Augmentation
inter-mixup: True

inter-mixup-layer: -1
inter-mixup-decoder-layer: 0
inter-mixup-prob: 1.0
inter-mixup-ratio: 1.0
inter-mixup-beta: 0.2

inter-mixup-keep-org: True
inter-mixup-decoder-emb: True

cal-mixup-loss: False
no-specaugment: False
layer-out-norm: False

inter-mixup-ratio-decay: False
inter-mixup-ratio-decay-params: 20000,40000,0

# MTL
inter-ctc-weight: 1.0
inter-ctc-layers: 6,9,12,15
share-inter-ctc: True
share-ctc-and-embed: True

ctc-pae: inter_league
pae-unnorm-input: True

ctc-mixup-consistent-weight: 0.15
inter-ctc-mixup-consistent-weight: 0.1
mixup-consistent-weight: 0.5

# Conformer
macaron-style: True
use-cnn-module: True
cnn-module-kernel: 15
encoder-attention-type: rel_pos
encoder-activation-fn: swish
layer-padding-mask: True